{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Set #3: Models of Reinforcement Learning\n",
    "**DUE 27 April 2021 - NEU 502A**\n",
    "\n",
    "**Submission Directions:** Prior to submitting, clear you kernel and run through all your cells in order so figures and variables are accurately recorded. Only include cells with necessary code or answers, do not include cells used for troubleshooting. Email this notebook with your last name in the filename to Eleni (elenip@princeton.edu) by 5pm.\n",
    "\n",
    "Previously we aimed to get a sense of how a system could extract the hidden structure in sensory stimuli (i.e. natural scenes). We implemented unsupervised learning in the form of Hebbian rules to this end. Our model was shown a set of images with no final goal specified, nor expectations with which to compare its performance throughout learning. Now we are interested in how a system can learn to reach a goal through interactions with its environment. Specifically, we are interested in modelling how a system can learn to maximize rewards or minimize penalties.\n",
    "\n",
    "Think about an infant as it moves and looks around. Sometimes the parent will guide its actions, yet it may not be the case at all times. Even when alone, there is learning taking place. There is a sensory-motor connection with the environment, where the infant gains knowledge about cause and effect. Throughout our experience, we are aware of how our surroundings respond to our actions. Thus, we actively seek to influence this relationship, and learn from it to maximize positive interactions and minimize negative ones.\n",
    "    \n",
    "Reinforcement learning models are about solving problems to maximize a quantification of reward. The models use goal-directed learning to solve closed-loop¬ problems. This can be seen below where present actions influence the environment, in turn changing the circumstances in which the learning system will act in the future towards the same goal. In RL, we hope to discover the actions that increase chances of rewards within specific states in the environment.\n",
    "\n",
    "<img src=\"model.png\" width=\"350\">  \n",
    "\n",
    "Let’s call the learning system an agent interacting with an environment to achieve a goal. This agent must be able to sense the state of the environment either fully or partially, and its actions must be able to change this said state. Here is an example:\n",
    "> “Think about preparing breakfast. Closely examined, this activity reveals a complex web of conditional behaviors with goal-subgoal relationships: walking to the cupboard, opening it, selecting a cereal box, then reaching for, grasping, and retrieving the box. Another set of interactive sequences are required to obtain a bowl, spoon, and milk jug. Each step involves a series of eye movements to obtain information and to guide them. Rapid judgments are continually made about how to carry the objects or whether it is better to ferry some of them to the dining table before obtaining others. Each step is guided by goals, such as grasping a spoon or getting to the refrigerator, and is in service of other goals, such as having the spoon to eat with once the cereal is prepared and ultimately obtaining nourishment. Whether you are aware of it or not, you are accessing information about the state of your body that determines nutritional needs, level of hunger, and food preferences.”\n",
    "\n",
    "At each junction, there is a state-action pair. Some of them fall under sub-goals, while others could ultimately be a state where there is a high chance of reward, fulfilling the goal of feeding. To be able to model this process, we have to break it down into its interacting components: \n",
    "- The agent has a policy, the map between perceived states and the actions taken. We can think of it as a set of stimulus-response rules or associations that determine behavior given a state and a goal within the environment. It can be implemented through the probabilities of taking specific actions given a state. \n",
    "- This set of rules should help to maximize the reward signal, be it at the short or long term. \n",
    "- The signal is evaluated through a value function, which provides a measure of the expected rewards that can be obtained moving forward from a specific state. Grabbing a bowl might not feed you immediately, yet it has high value as it will lead you to a state in which you can feed yourself some cereal without spilling milk all over the table. Would grabbing a shallow dish instead of a bowl have the same value, within the confines of the above example? Actions are taken based on these value judgements. \n",
    "- The agent could have the ability for foresight and planning if it has a model of the environment. This means it can have a model of how the environment reacts to its behavior, from which to base its strategies and adjustments.\n",
    "\n",
    "At each decision, the agent has a choice to either exploit the actions it has already tested to be effective, or it can explore the action-state space to find new routes to optimal rewards. Exploration is risky, yet under some circumstances it will pay off in the long run. Finding the balance between the two would be the optimal solution in uncertain environments. Different methods can be employed to deal with this duality:\n",
    "- On-policy methods improve the policy that is used to make decisions. This policy is generally soft (probabilistic), as $P(s∈S,a∈A│s)>0$, where $S$ is the possible states and $A|s$ is the possible actions given a state. The probability is gradually shifted to a deterministic optimal policy with each update. For example, $\\epsilon-greedy$ policies choose an action that has maximal expected value most of the time (with probability 1 – a small number $\\epsilon$). However, with probability $\\epsilon$ they choose an action at random. The method is trying to learn values based on subsequent optimal behavior, yet it has to behave non-optimally (choosing random actions) in order to explore and find the optimal actions. This means the agent has to learn about the optimal policy while behaving according to an exploratory policy. On-policy can be thought of as a compromise, where values are learned for a near optimal policy that still explores. \n",
    "- Another approach is to use two policies, a target policy and a behavior policy. The first one is modified based on the information that is collected through the behaviors generated by the second. This approach is termed off-policy, as learning occurs based on behaviors generated off the policy being developed. The benefit here is that the target policy can be deterministic (i.e. greedy), while the behavior-policy can continue to explore without limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import loadmat\n",
    "from scipy.optimize import minimize\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_context('talk')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART I – Fitting RL models to data\n",
    "\n",
    "**A :: Run the *sequential/sequential.m* matlab task on yourself.**\n",
    "\n",
    "First, we need to acquire some data. You are the agent this time around. For this you need Matlab with Psychtoolbox installed. The code that runs the experiment has been tested on Mac. Some adjustments may be needed so that it runs smoothly on Windows. (If you are running Windows, we recommend you borrow a Mac to do the experiment, or use the Princeton Mac cluster). If you are unable to run the experiment, please inform the instructor. \n",
    "\n",
    "Steps to run:\n",
    "\n",
    "1.\tUnzip the attached directory “/sequential”\n",
    "\n",
    "2.\tRun “sequential/tutorial.m” which will explain the decision-making task.\n",
    "\n",
    "3.\tRun the task itself: “sequential/sequential.m” - this is a challenging task, so you might want to sequester yourself to a quiet room where you can concentrate. The task will take about 20 minutes. Try to do your best as you will compete to see who wins the most coins! \n",
    "\n",
    "    The data from your experiment can be found under the directory from which you ran the task, in a file called “Subj##-date-time.mat” where ## is the subject number you gave yourself, and the date and time are those at which you finished the task. (Please Email your data to the A.I. when you are done). The data is structured as follows:\n",
    "•\tchoice1 - your choices at the first level (1 or 2)\n",
    "•\tchoice2 - your choices at the second level (1 or 2)\n",
    "•\tstate - which second level game you were offered on this trial (choice1 = 1 at the first level (S1) leads to S2 in approximately 70% of the trials; choice1 = 2 leads to S3 in approximately 70% of the trials)\n",
    "•\tmoney - did you get a reward on each trial or not (1 or 0)\n",
    "\n",
    "    Note that missed trials will have a 0 in the choice; trials can be missed either at the first or second level. When you write your code (latter on), make sure to deal separately with missed trials as this is a common source of discrepancies while fitting the models. Below is a schematic of the task structure:\n",
    "\n",
    "\n",
    "<img src=\"task.png\" width=\"400\">\n",
    "\n",
    "|       | $A_1$     | $A_2$     |\n",
    "|:-----:|:---------:|:---------:|\n",
    "| $S_1$ | $Q_{S1,A1}$ | $Q_{S1,A2}$ |\n",
    "| $S_2$ | $Q_{S2,A1}$ | $Q_{S2,A2}$ |\n",
    "| $S_3$ | $Q_{S3,A1}$ | $Q_{S3,A2}$ |\n",
    "\n",
    "\n",
    "The schematic might not map to the colors used in the actual task.  S1 refers to the state at the top (first) level, where you will be shown two distinct symbols. You will have to choose one of the two (represented by action | state in the schematic). One of the symbols, let’s say A1|S1 will have 70% chance of transferring you to S2 (one of the possible states at the bottom level), and a 30% chance of getting you to S3. This is represented by the thickness of the arrows. The same goes for the symbol represented by A2|S1, yet the chances are inverted. At the bottom (second) level, you can be at either of two distinct states (S2 or S3). Once again you will need to choose between two symbols at each state, with gradually drifting chances of getting a coin once a decision is made. For example, symbol A1|S2 might start with higher chances than A2|S2. These will change gradually with time and at some point, the chances might be reversed. There is no implicit relationship between what happens in S2 and S3. You will have to learn, with each experience, which choices lead you to better rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(fn):\n",
    "    \"Loading in matlab data structures for 2-step RL task (Daw et al., 2011)\"\n",
    "    d = loadmat(fn)\n",
    "    \n",
    "    def fix_vector(arr):\n",
    "        \"Making the array signed for subtraction to work, and resizing to 2D matrix and last dim size is 1\"\n",
    "        return arr.astype(np.int8).squeeze().reshape((-1,1))\n",
    "    \n",
    "    #variables for fitting:\n",
    "    choice1 = fix_vector(d[\"choice1\"])\n",
    "    choice2 = fix_vector(d[\"choice2\"])\n",
    "    state = fix_vector(d[\"state\"])\n",
    "    money = fix_vector(d[\"money\"])\n",
    "    \n",
    "    #adjust to start 0-indexing\n",
    "    choice1 -= 1\n",
    "    choice2 -= 1\n",
    "    state -= 1\n",
    "    \n",
    "    #adjust to 1D vectors\n",
    "    return choice1.flatten(), choice2.flatten(), state.flatten(), money.flatten()\n",
    "\n",
    "#load given sample data\n",
    "c1, c2, s, m = load_data('data.mat')\n",
    "\n",
    "#load collected class data. given to you by your AI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B :: Write down the update (learning) equations for two different models:**\n",
    "\n",
    "**b1.** At the bottom level, learning can be modeled with $Q$-learning or Rescorla-Wagner learning (as there is no future state). Note that these are identical if you treat each option as an action (in $Q$ learning) or as a state (the state of the chosen stimulus; in Rescorla-Wagner). Write down the equation for either one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Rescorla-Wagner**: $\\Delta V = \\eta (R- V(s))$\n",
    ">\n",
    ">or\n",
    ">\n",
    ">**Q-Learning**: $Q(s, a) \\leftarrow Q(s, a) + \\eta\\lambda$, where $\\lambda = R + \\gamma Q_{max(a)}(s', a)- Q(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b2.** At the top level, you can learn in several different ways. We will consider two:\n",
    "\n",
    "**b2a.** Model-free learning:\n",
    "\n",
    "> **b2ai.** Write down the Temporal Difference (TD) update rule for the first level. Can you think of a way to improve upon this learning rule, without requiring a model of the environment? For instance, how can you take into account the result of the choice at the second level here? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>**TD(0)**: $Q(s,a) \\leftarrow Q(s,a) + \\eta\\lambda$, where $\\lambda = R + Q(s', a') - Q(s, a)$\n",
    ">>\n",
    ">>**Answer:** TEXT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **b2aii.** One way to make learning more efficient is to use TD($\\lambda$) instead of TD(0) learning. In this case, there is an additional memory variable associated with each state, its “eligibility trace\". You can think of it as a “memory\" that a particular state has been visited, which decays (for instance, exponentially) over time. Every time a state is visited, its eligibility trace becomes 1; at every subsequent time-point the eligibility trace is multiplied by a factor $0 < \\lambda \\leq 1$. At the end of a trial or episode, all eligibility traces become 0. \n",
    ">\n",
    "> All states are updated according to *learning rate $\\cdot$ prediction error $\\cdot$ eligibility trace*. This will automatically update all the states visited in this episode (i.e. all the states `eligible' for updating), doing so for the most recently visited states to a greater extent. Write the updated equation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>**TD($\\lambda$)**: EQUATION HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b2b.** Model-based learning:\n",
    "\n",
    "> **b2bi.** Assume that the transition model (i.e. the probabilities of going from $S1$ to $S2$ or $S3$ given choice1) is known from the start, while the reward model is not known. How can you use the transitions and the learned values at the bottom level to plan and make choices at the top level?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>**Answer:** TEXT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**b2bii.** How would you implement it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>**Answer:** TEXT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b3.** How many parameters do each of these models have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**RW:** \n",
    ">\n",
    ">**Q-Learning:** \n",
    ">\n",
    ">**TD(0):** \n",
    ">\n",
    ">**TD($\\lambda$):** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C :: Implement and fit the above models.** Below, we have provided a template function rl_nll() for computing the negative log-likelihood (LL) of the data and code blocks with templates computing LL given parameters and for fitting the models using Scipy's minimize(). You will refer to these templates to complete later questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of partial rl_nll() function\n",
    "\n",
    "def rl_nll(eta, beta, lambd, state, choice1, choice2, money):\n",
    "    \"\"\"\n",
    "    output: LL - the negative log likelihood of the data\n",
    "    input:\n",
    "        eta     - learning rate\n",
    "        beta    - softmax inverse temperature\n",
    "        lambda  - eligibility trace decay rate (set to 0 to get TD(0) without eligibility traces)\n",
    "        state   - state, 0 is the top level, 1 and 2 are the bottom level\n",
    "        choice1 - the choice at the top level -- 0 or 1 (-1 for missed trials)\n",
    "        choice2 - the choice at the bottom level -- 0 or 1 (-1 for missed trials)\n",
    "        money   - amount won (1 or 0)\n",
    "    \"\"\"\n",
    "    \n",
    "    n_states = 3\n",
    "    n_actions = 2\n",
    "    n_trials = len(choice1)\n",
    "    \n",
    "    Q = np.zeros([n_states, n_actions]) # initialize Q values to 0\n",
    "    LL = 0.0 # initialize log-likelihood\n",
    "    \n",
    "    # loop through trials\n",
    "    for t in range(n_trials):\n",
    "        \n",
    "        # initialize eligibility trace\n",
    "        E = np.zeros([n_states, n_actions])\n",
    "        S = 0 # current state, first we are in the top level \n",
    "        \n",
    "        # stop if trial was missed \n",
    "        if choice1[t] == -1:\n",
    "            continue\n",
    "                \n",
    "        # first level choice likelihood (compute likelihood of choice at first state s)\n",
    "        # note: this comment uses Matlab notation, copied from rllik.m \n",
    "        # p(1) = exp(b(q(s, a)/sum(exp(b(q(s, all_actions))))\n",
    "        # p(2) = 1 - p(1) \n",
    "        # p(chosen) = p(x) either 1 or 2 - look at data & compare\n",
    "        # LL = LL + log(p(chosen))\n",
    "        LL += # LL is log-likelihood of choices, fill something in here \n",
    "\n",
    "        # learning at first level \n",
    "        E[S, choice1[t]] = 1 # eligibility trace updated\n",
    "        PE = # prediction error, fill in something here\n",
    "        \n",
    "        Q[S, choice1[t]] = # fill in something here to update Q values\n",
    "        \n",
    "        # second level choice likelihood\n",
    "        S = state[t] \n",
    "        \n",
    "        # stop if trial was missed\n",
    "        if choice2[t] == -1:\n",
    "            continue\n",
    "            \n",
    "        # update LL with second choice log likelihood\n",
    "        LL += # fill in something here\n",
    "        \n",
    "        # learning at second level\n",
    "        \n",
    "        # update eligibility trace\n",
    "        E = lambd * E # first decay all eligbility traces\n",
    "        E[S, choice2[t]] = 1 # then update current eligilibity trace\n",
    "        \n",
    "        PE = # fill in something here\n",
    "        \n",
    "        # update Q values \n",
    "        Q # fill in something here\n",
    "        \n",
    "        # Is this model-based or model-free? Do you have to update first level again?\n",
    "        # if you need to update first level again, do so here. \n",
    "        \n",
    "    # we are minimizing this function, so use negative LL\n",
    "    LL *= -1.0\n",
    "    \n",
    "    return LL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of computing log-likelihood given parameters eta, beta, and lambd \n",
    "# note that this will not run until rl_nll is completed\n",
    "\n",
    "# initialize parameters\n",
    "eta = 0.5\n",
    "beta = 0.5\n",
    "lambd = 0.5\n",
    "params = np.array([eta,beta,lambd])\n",
    "\n",
    "print(\"model-free LL of given sample data:\", rl_nll(params, s, c1, c2, m, model_based=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of constrained optimization of parameters \n",
    "# note that this will not run until rl_nll is completed\n",
    "\n",
    "# initial condition\n",
    "x0 = np.random.uniform(size=[3,])\n",
    "\n",
    "# boundaries for eta, beta, and lambd respectively\n",
    "eta_bnd = (LB, UB)\n",
    "beta_bnd = (LB, UB)\n",
    "lambd_bnd = (LB, UB)\n",
    "bnds = (eta_bnd, beta_bnd, lambd_bnd)\n",
    "\n",
    "# optimize by minimizing the negative log likelihood\n",
    "# xo is initial condition of parameters\n",
    "# optimization method is L-BFGS-B\n",
    "# bounds are the lower and upper bounds\n",
    "# args are arguments to the function we are optimizing after the first argument, which is the parameters\n",
    "res = minimize(rl_nll, x0, method='L-BFGS-B', bounds=bnds, \n",
    "               args=(state, choice1, choice2, money))\n",
    "\n",
    "# print results\n",
    "print(\"Params [eta, beta, lambda] = \",res.x) # the final setting of the parameters\n",
    "print(\"Negative log likelihood = \", res.fun) # the final negative log-likelihood value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c1.** Using the template above, modify the log-likelihood function rl_nll() so that it implements model-free and model-based learning by including the input: model_based=True or model_based=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_nll(eta, beta, lambd, state, choice1, choice2, money, model_based=False):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    return LL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c1a.** (Model-free). Implement TD($\\lambda$) using the $Q$-learning or State-Action-Reward-State-Action (SARSA) algorithms. These algorithms use state-action value predictions ($Q$ values) to choose actions. In state $S$, they choose an action a according to softmax $Q$ values:\n",
    "\n",
    "<img src=\"Pas.png\" width=\"500\"> \n",
    "\n",
    "Here, $β$ is called an inverse-temperature parameter that we will optimize. If you are using constrained optimization, fix $β$ to be in the range [0, 100].\n",
    "\n",
    ">**c1ai.** Update the eligibility traces. Recall that the eligibility traces are values corresponding to each state and action pair, and are set to zero at the beginning of the trial. Upon taking action $a$ and leaving state $S$ for state $S^{new}$, and receiving reward $r$ the eligibility traces $e(a│S)$ are updated for each $(S, a)$ pair:\n",
    ">\n",
    "> <img src=\"eaS.png\" width=\"300\">\n",
    ">\n",
    ">**c1aii.** All the $Q(a|S)$ are updated according to:\n",
    ">\n",
    "> <img src=\"Q.png\" width=\"250\">\n",
    ">\n",
    "> With the prediction error $δ(t)$ being: \n",
    ">\n",
    "> <img src=\"delta.png\" width=\"350\">\n",
    ">\n",
    "> The parameter $η$ is a step-size or learning-rate parameter in the range (0,1]. \n",
    ">\n",
    ">**c1aiii.** Reset the eligibility traces to 0 at the end of each round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**c1aiv.** Which of these two algorithms is considered on-policy, which is off-policy, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>**Answer:** TEXT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c1b.** (Model-based). Implement your model-based algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c2.** Write a function that will find the maximum-likelihood settings of the parameters of the models. Modify the example optimization code above to fit the model free and model based models. The example code uses SciPy’s minimize optimizer, and is commented in the template above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c3.** Use the optimization function you edited to fit the two models above to your data and that of your classmates (provided by the AI). Load your data and then run the fitting routine 5 times, starting from 5 random initial values for the parameters to be fit. After running all 5 times, choose the best fit (minimum negative log likelihood, or maximum log likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.random.seed(###) use random.seed to keep values consistent\n",
    "\n",
    "#run fitting routine 5 times:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deterimine best fit:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c4.** Use Bayesian Information Criterion (BIC) to compare which is the best fitting model for each subject. Compute the BIC using the formula: \n",
    "\n",
    "BIC = -2 * log-likelihood + ln(trials) *  # parameters\n",
    "\n",
    "where $ln()$ is the natural logarithm. BIC is defined here on the deviance scale, which means that lower values are better. Which model fit each subject’s behavior best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:** TEXT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART II – Cliff walking\n",
    "\n",
    "Consider the grid world shown below. This is a standard undiscounted, episodic task, with start and goal states, and the usual actions causing movement up, down, right, and left. Reward is -1 on all transitions except those into the region marked “The Cliff.” Stepping into this region incurs a reward of -100 and sends the agent instantly back to the start.\n",
    "\n",
    "<img src=\"cliff.png\" width=\"500\">  \n",
    "\n",
    "Two paths are marked: an optimal path which incurs the least costs on the way to the goal, and a roundabout (but safe) path that walks farthest from the cliff. \n",
    "\n",
    "**1.** Which algorithm, SARSA or $Q$-learning, would learn either path, and why? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:** TEXT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.** When behaving according to softmax of the learned $Q$ values, which path would an agent prefer? (think about the parameter $β$ and the stability of the environment) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:** TEXT HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.** Can you suggest why on-policy methods might be superior for learning real-world motor behavior?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**Answer:** TEXT HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
